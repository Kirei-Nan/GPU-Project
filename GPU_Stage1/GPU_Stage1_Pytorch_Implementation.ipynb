{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c3be5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanzhenghan/opt/anaconda3/lib/python3.9/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/nanzhenghan/opt/anaconda3/lib/python3.9/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanzhenghan/opt/anaconda3/lib/python3.9/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total dataset statistics:\n",
      "Total samples: 25000\n",
      "Total positive reviews: 12500\n",
      "Total negative reviews: 12500\n",
      "\n",
      "Inspecting 10 random samples:\n",
      "\n",
      "Sample 1:\n",
      "True Label (raw): 1\n",
      "Sentiment: Negative\n",
      "Review preview: no redeeming qualities can possibly be expressed. i wish i could get my time back. nice skull face broad really smiles, bright at the camera when the disease has already wreaked enough havoc on the il...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "True Label (raw): 2\n",
      "Sentiment: Positive\n",
      "Review preview: \"A Guy Thing\" may not be a classic, but it sure is a good, funny comedy. The plot focuses on Paul (Jason Lee), who wakes up the morning after his bachelor party with no memory and Becky (Julia Stiles)...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 3:\n",
      "True Label (raw): 1\n",
      "Sentiment: Negative\n",
      "Review preview: I'm just quite disappointed with \"Soul Survivors\". It doesn't worth even a comment in this forum. The script is very poor as well as all the \"acting\" and for our entertainment it features a pointless ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 4:\n",
      "True Label (raw): 2\n",
      "Sentiment: Positive\n",
      "Review preview: They probably could have skipped some of the beginning - I'm not sure why this starts out in the Asian part of Turkey. If it was because starting in the Mediterranean, they could have gotten closer st...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 5:\n",
      "True Label (raw): 2\n",
      "Sentiment: Positive\n",
      "Review preview: Forget the campy 'religious' movies that have monopolized the television/film market... this movie has a real feel to it. While it may be deemed as a movie that has cheap emotional draws, it also has ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 6:\n",
      "True Label (raw): 1\n",
      "Sentiment: Negative\n",
      "Review preview: this video is 100% retarded. besides the brain cell killing acting and plot, it's way too long. don't waste your money at the video store. i actually was mad that i sat through this garbage and spent ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 7:\n",
      "True Label (raw): 2\n",
      "Sentiment: Positive\n",
      "Review preview: I have just seen this film, and fallen in love with it. There is a little bit of something for everyone, and its a particular free for all when it comes to the romance between Lachlan and Lil. When th...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 8:\n",
      "True Label (raw): 2\n",
      "Sentiment: Positive\n",
      "Review preview: I saw a trailer for this on Afro Promo, the collection of movie trailers for movies featuring African-Americans. It looked like what it is; a highly tendentious \"wacky\" comedy in which an uptight blac...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 9:\n",
      "True Label (raw): 1\n",
      "Sentiment: Negative\n",
      "Review preview: *****Warning: May contain SPOILERS********* My HUGE problem with this movie is how totally self-centered and self consumed the adulteress wife is!! After having a one night stand with a slimy psycho s...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 10:\n",
      "True Label (raw): 2\n",
      "Sentiment: Positive\n",
      "Review preview: What can one say about Elvira that hasn't already been said in the world's press? The classic comedienne that IS Elvira delivers in her first full-length big budget comedy masterpiece.<br /><br />From...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "In these random samples:\n",
      "Positive reviews: 6\n",
      "Negative reviews: 4\n"
     ]
    }
   ],
   "source": [
    "# Check IMDB dataset format\n",
    "import torch\n",
    "from torchtext.datasets import IMDB\n",
    "import torchtext\n",
    "import random\n",
    "\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "def inspect_imdb_data():\n",
    "    print(\"Loading IMDB dataset...\")\n",
    "    train_data = IMDB(split='train')\n",
    "    \n",
    "    # Convert to list and count overall distribution\n",
    "    all_samples = list(train_data)\n",
    "    total_pos = sum(1 for label, _ in all_samples if label == 2)\n",
    "    total_neg = sum(1 for label, _ in all_samples if label == 1)\n",
    "    \n",
    "    print(f\"\\nTotal dataset statistics:\")\n",
    "    print(f\"Total samples: {len(all_samples)}\")\n",
    "    print(f\"Total positive reviews: {total_pos}\")\n",
    "    print(f\"Total negative reviews: {total_neg}\")\n",
    "    \n",
    "    # Randomly select 10 samples for inspection\n",
    "    print(\"\\nInspecting 10 random samples:\")\n",
    "    random_samples = random.sample(all_samples, 10)\n",
    "    \n",
    "    for i, sample in enumerate(random_samples, 1):\n",
    "        print(f\"\\nSample {i}:\")\n",
    "        true_label, review_text = sample\n",
    "        \n",
    "        sentiment = \"Negative\" if true_label == 1 else \"Positive\"\n",
    "        \n",
    "        print(f\"True Label (raw): {true_label}\")\n",
    "        print(f\"Sentiment: {sentiment}\")\n",
    "        print(f\"Review preview: {review_text[:200]}...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "    # Calculate distribution in random samples\n",
    "    sample_pos = sum(1 for label, _ in random_samples if label == 2)\n",
    "    sample_neg = sum(1 for label, _ in random_samples if label == 1)\n",
    "    print(f\"\\nIn these random samples:\")\n",
    "    print(f\"Positive reviews: {sample_pos}\")\n",
    "    print(f\"Negative reviews: {sample_neg}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inspect_imdb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ccd9c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n",
      "Loading IMDb dataset...\n",
      "Loading GloVe vectors...\n",
      "Creating data loaders...\n",
      "Preprocessing train dataset...\n",
      "Separating positive and negative samples for train set...\n",
      "Total train samples - Positive: 12500, Negative: 12500\n",
      "Using 500 samples per class for train set\n",
      "Processing train samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1012.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 1000 train samples\n",
      "Final train distribution - Positive: 500, Negative: 500\n",
      "Preprocessing test dataset...\n",
      "Separating positive and negative samples for test set...\n",
      "Total test samples - Positive: 12500, Negative: 12500\n",
      "Using 100 samples per class for test set\n",
      "Processing test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 200/200 [00:00<00:00, 1193.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 200 test samples\n",
      "Final test distribution - Positive: 100, Negative: 100\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|█████| 125/125 [00:22<00:00,  5.65it/s, loss=0.6893, acc=52.20%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Loss: 0.6893\n",
      "Overall Accuracy: 52.20%\n",
      "Positive Reviews Accuracy: 72.60%\n",
      "Negative Reviews Accuracy: 31.80%\n",
      "New best accuracy: 52.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|█████| 125/125 [00:21<00:00,  5.68it/s, loss=0.6038, acc=68.20%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Loss: 0.6038\n",
      "Overall Accuracy: 68.20%\n",
      "Positive Reviews Accuracy: 64.60%\n",
      "Negative Reviews Accuracy: 71.80%\n",
      "New best accuracy: 68.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|█████| 125/125 [00:22<00:00,  5.60it/s, loss=0.4904, acc=77.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Loss: 0.4904\n",
      "Overall Accuracy: 77.80%\n",
      "Positive Reviews Accuracy: 78.00%\n",
      "Negative Reviews Accuracy: 77.60%\n",
      "New best accuracy: 77.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|█████| 125/125 [00:22<00:00,  5.55it/s, loss=0.3932, acc=82.20%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Loss: 0.3932\n",
      "Overall Accuracy: 82.20%\n",
      "Positive Reviews Accuracy: 82.20%\n",
      "Negative Reviews Accuracy: 82.20%\n",
      "New best accuracy: 82.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|█████| 125/125 [00:22<00:00,  5.56it/s, loss=0.3664, acc=83.70%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Loss: 0.3664\n",
      "Overall Accuracy: 83.70%\n",
      "Positive Reviews Accuracy: 86.60%\n",
      "Negative Reviews Accuracy: 80.80%\n",
      "New best accuracy: 83.70%\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 25/25 [00:01<00:00, 21.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Loss: 0.4308\n",
      "Overall Accuracy: 80.50%\n",
      "Positive Reviews Accuracy: 75.00%\n",
      "Negative Reviews Accuracy: 86.00%\n",
      "Saving model weights...\n",
      "Saved feature_extractor.0.weight to weights/feature_extractor_0_weight.txt\n",
      "Saved feature_extractor.0.bias to weights/feature_extractor_0_bias.txt\n",
      "Saved feature_extractor.3.weight to weights/feature_extractor_3_weight.txt\n",
      "Saved feature_extractor.3.bias to weights/feature_extractor_3_bias.txt\n",
      "Saved classifier.0.weight to weights/classifier_0_weight.txt\n",
      "Saved classifier.0.bias to weights/classifier_0_bias.txt\n",
      "Saved classifier.3.weight to weights/classifier_3_weight.txt\n",
      "Saved classifier.3.bias to weights/classifier_3_bias.txt\n",
      "Saved feature_extractor.0.weight to weights_bin/feature_extractor_0_weight.bin\n",
      "Saved feature_extractor.0.bias to weights_bin/feature_extractor_0_bias.bin\n",
      "Saved feature_extractor.3.weight to weights_bin/feature_extractor_3_weight.bin\n",
      "Saved feature_extractor.3.bias to weights_bin/feature_extractor_3_bias.bin\n",
      "Saved classifier.0.weight to weights_bin/classifier_0_weight.bin\n",
      "Saved classifier.0.bias to weights_bin/classifier_0_bias.bin\n",
      "Saved classifier.3.weight to weights_bin/classifier_3_weight.bin\n",
      "Saved classifier.3.bias to weights_bin/classifier_3_bias.bin\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.datasets import IMDB\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "import torchtext\n",
    "import random\n",
    "\n",
    "# Disable torchtext deprecation warnings\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Text preprocessing function\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Truncate or pad to fixed length\n",
    "    if len(tokens) > 256:\n",
    "        tokens = tokens[:256]\n",
    "    else:\n",
    "        tokens.extend(['<pad>'] * (256 - len(tokens)))\n",
    "    return tokens\n",
    "\n",
    "def text_to_features(tokens, glove):\n",
    "    # Convert tokens to GloVe embeddings\n",
    "    features = torch.zeros(256, 300)\n",
    "    for i, token in enumerate(tokens[:256]):\n",
    "        if token in glove.stoi:\n",
    "            features[i] = glove.vectors[glove.stoi[token]]\n",
    "    return features\n",
    "\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, data_iter, glove, max_samples=None, split='train'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            data_iter: IMDB data iterator\n",
    "            glove: GloVe word vectors\n",
    "            max_samples: Maximum samples per class (None means use all data)\n",
    "            split: 'train' or 'test', used for display information\n",
    "        \"\"\"\n",
    "        print(f\"Preprocessing {split} dataset...\")\n",
    "        # Collect all samples and group by label\n",
    "        pos_samples = []\n",
    "        neg_samples = []\n",
    "        all_data = list(data_iter)\n",
    "        \n",
    "        print(f\"Separating positive and negative samples for {split} set...\")\n",
    "        for label, text in all_data:\n",
    "            if label == 1:  # negative\n",
    "                neg_samples.append((label, text))\n",
    "            else:  # positive (label == 2)\n",
    "                pos_samples.append((label, text))\n",
    "                \n",
    "        print(f\"Total {split} samples - Positive: {len(pos_samples)}, Negative: {len(neg_samples)}\")\n",
    "        \n",
    "        # If max_samples specified, ensure balanced classes\n",
    "        if max_samples:\n",
    "            samples_per_class = max_samples // 2\n",
    "            pos_samples = random.sample(pos_samples, samples_per_class)\n",
    "            neg_samples = random.sample(neg_samples, samples_per_class)\n",
    "            print(f\"Using {samples_per_class} samples per class for {split} set\")\n",
    "            \n",
    "        # Combine and shuffle samples\n",
    "        self.samples = []\n",
    "        combined_samples = pos_samples + neg_samples\n",
    "        random.shuffle(combined_samples)\n",
    "        \n",
    "        # Process samples\n",
    "        print(f\"Processing {split} samples...\")\n",
    "        for label, text in tqdm(combined_samples):\n",
    "            try:\n",
    "                tokens = preprocess_text(text)\n",
    "                features = text_to_features(tokens, glove)\n",
    "                label_tensor = torch.tensor(label - 1, dtype=torch.long)  # Convert 1->0, 2->1\n",
    "                self.samples.append((features, label_tensor))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample: {str(e)[:200]}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully processed {len(self.samples)} {split} samples\")\n",
    "        \n",
    "        # Verify final distribution\n",
    "        final_pos = sum(1 for _, label in self.samples if label == 1)\n",
    "        final_neg = sum(1 for _, label in self.samples if label == 0)\n",
    "        print(f\"Final {split} distribution - Positive: {final_pos}, Negative: {final_neg}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        # Feature extraction layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(300, 150),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(150, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        token_features = []\n",
    "        # Process each token in the sequence\n",
    "        for i in range(x.size(1)):\n",
    "            token_vec = x[:, i, :]\n",
    "            token_features.append(self.feature_extractor(token_vec))\n",
    "        \n",
    "        token_features = torch.stack(token_features, dim=1)\n",
    "        # Mean pooling over token features\n",
    "        pooled_features = torch.mean(token_features, dim=1)\n",
    "        \n",
    "        output = self.classifier(pooled_features)\n",
    "        return output\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate model performance on test data\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pos_correct = 0\n",
    "    neg_correct = 0\n",
    "    pos_total = 0\n",
    "    neg_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Track accuracy separately for positive and negative samples\n",
    "            pos_mask = labels == 1\n",
    "            neg_mask = labels == 0\n",
    "            pos_correct += (predicted[pos_mask] == labels[pos_mask]).sum().item()\n",
    "            neg_correct += (predicted[neg_mask] == labels[neg_mask]).sum().item()\n",
    "            pos_total += pos_mask.sum().item()\n",
    "            neg_total += neg_mask.sum().item()\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    pos_accuracy = 100 * pos_correct / pos_total if pos_total > 0 else 0\n",
    "    neg_accuracy = 100 * neg_correct / neg_total if neg_total > 0 else 0\n",
    "    \n",
    "    print('Test Results:')\n",
    "    print(f'Loss: {test_loss:.4f}')\n",
    "    print(f'Overall Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Positive Reviews Accuracy: {pos_accuracy:.2f}%')\n",
    "    print(f'Negative Reviews Accuracy: {neg_accuracy:.2f}%')\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    model.train()\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pos_correct = 0\n",
    "        neg_correct = 0\n",
    "        pos_total = 0\n",
    "        neg_total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Track accuracy by class\n",
    "            pos_mask = labels == 1\n",
    "            neg_mask = labels == 0\n",
    "            pos_correct += (predicted[pos_mask] == labels[pos_mask]).sum().item()\n",
    "            neg_correct += (predicted[neg_mask] == labels[neg_mask]).sum().item()\n",
    "            pos_total += pos_mask.sum().item()\n",
    "            neg_total += neg_mask.sum().item()\n",
    "            \n",
    "            # Clean up GPU memory if using MPS\n",
    "            if device.type == 'mps':\n",
    "                torch.mps.empty_cache()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{running_loss/(total//labels.size(0)):.4f}',\n",
    "                'acc': f'{100*correct/total:.2f}%'\n",
    "            })\n",
    "            \n",
    "        # Print epoch statistics\n",
    "        epoch_acc = 100 * correct/total\n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f'Loss: {running_loss/len(train_loader):.4f}')\n",
    "        print(f'Overall Accuracy: {epoch_acc:.2f}%')\n",
    "        print(f'Positive Reviews Accuracy: {100*pos_correct/pos_total:.2f}%')\n",
    "        print(f'Negative Reviews Accuracy: {100*neg_correct/neg_total:.2f}%')\n",
    "        \n",
    "        if epoch_acc > best_accuracy:\n",
    "            best_accuracy = epoch_acc\n",
    "            print(f'New best accuracy: {best_accuracy:.2f}%')\n",
    "\n",
    "def save_weights_txt(model, output_dir='weights'):\n",
    "    \"\"\"Save model weights in text format\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    state_dict = model.state_dict()\n",
    "    for name, param in state_dict.items():\n",
    "        param_numpy = param.detach().cpu().numpy()\n",
    "        filename = os.path.join(output_dir, f\"{name.replace('.', '_')}.txt\")\n",
    "        param_numpy.tofile(filename, sep='\\n', format='%.8e')\n",
    "        print(f\"Saved {name} to {filename}\")\n",
    "\n",
    "def save_weights_binary(model, output_dir='weights_bin'):\n",
    "    \"\"\"Save model weights in binary format\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    state_dict = model.state_dict()\n",
    "    for name, param in state_dict.items():\n",
    "        param_numpy = param.detach().cpu().numpy()\n",
    "        filename = os.path.join(output_dir, f\"{name.replace('.', '_')}.bin\")\n",
    "        param_numpy.tofile(filename)\n",
    "        print(f\"Saved {name} to {filename}\")\n",
    "\n",
    "def main(train_samples=1000, test_samples=200):\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    Parameters:\n",
    "        train_samples: Total number of training samples (will be split equally between positive/negative)\n",
    "        test_samples: Total number of test samples (will be split equally between positive/negative)\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Set device (MPS/CUDA/CPU)\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS device\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA device\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU device\")\n",
    "    \n",
    "    # Download NLTK data if needed\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        print(\"Downloading NLTK punkt tokenizer...\")\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    # Load datasets and word vectors\n",
    "    print(\"Loading IMDb dataset...\")\n",
    "    train_data = IMDB(split='train')\n",
    "    test_data = IMDB(split='test')\n",
    "    \n",
    "    print(\"Loading GloVe vectors...\")\n",
    "    glove = GloVe(name='6B', dim=300)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    print(\"Creating data loaders...\")\n",
    "    train_dataset = IMDbDataset(train_data, glove, max_samples=train_samples, split='train')\n",
    "    test_dataset = IMDbDataset(test_data, glove, max_samples=test_samples, split='test')\n",
    "    \n",
    "    if len(train_dataset) == 0 or len(test_dataset) == 0:\n",
    "        print(\"No valid samples were processed. Exiting...\")\n",
    "        return\n",
    "        \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    print(\"Initializing model...\")\n",
    "    model = SentimentModel().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        train_model(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        print(\"\\nEvaluating on test set...\")\n",
    "        test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        print(\"Try reducing batch size or model size if you're running out of memory\")\n",
    "        return\n",
    "        \n",
    "    # Save model weights\n",
    "    print(\"Saving model weights...\")\n",
    "    save_weights_txt(model)\n",
    "    save_weights_binary(model)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Modify these parameters to control training/test set sizes\n",
    "    main(train_samples=1000, test_samples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb09014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
